{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8a24cb",
   "metadata": {},
   "source": [
    "# Corpus Nummorum (CN) Data Challange Top 5 Accuarcies\n",
    "\n",
    "---\n",
    "\n",
    "**Goethe University Frankfurt am Main**\n",
    "\n",
    "Summer Semester 2023\n",
    "\n",
    "<br>\n",
    "\n",
    "## *Multimodal Fusion Model for  historical coin classification*\n",
    "\n",
    "---\n",
    "\n",
    "**Authors:** Bastian Rothenburger, Garegin Ktoian \n",
    "<br>\n",
    "\n",
    "**Contact:** Bastian Rothenburger ([s7072002@rz.uni-frankfurt.de](mailto:s7072002@rz.uni-frankfurt.de))<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650784bc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Table of Contents\n",
    "  - [Setup](#setup)\n",
    "- [1 Types](#1-types)\n",
    "  - [1.1 ResNet18 Base](#11-resnet18-base)\n",
    "    - [1.1.1 ImageEncoder Resnet18](#111-imageencoder-resnet18)\n",
    "    - [1.1.2 TextEncoder hidden size 10000](#112-textencoder-hidden-size-10000)\n",
    "    - [1.1.3 Fusion Model ResNet18 hidden size 10000](#113-fusion-model-resnet18-hidden-size-10000)\n",
    "  - [1.2 ResNet101 Base](#12-resnet101-base)\n",
    "    - [1.2.1 ImageEncoder ResNet101](#121-imageencoder-resnet101)\n",
    "    - [1.2.2 TextEncoder hidden size 12000](#122-textencoder-hidden-size-12000)\n",
    "    - [1.2.3 Fusion Model ResNet101 hidden size 12000](#123-fusion-model-resnet101-hidden-size-12000)\n",
    "- [2 Mints Multimodal](#2-mints-multimodal)\n",
    "  - [2.1 ResNet18 Base](#21-resnet18-base)\n",
    "    - [2.1.1 ImageEncoder ResNet18](#211-imageencoder-resnet18)\n",
    "    - [2.1.2 TextEncoder hidden size](#212-textencoder-hidden-size)\n",
    "    - [2.1.3 Fusion Model Resnet18 10000 hidden size](#213-fusion-model-resnet18-10000-hidden-size)\n",
    "  - [2.2 ResNet101 Base](#22-resnet101-base)\n",
    "    - [2.2.1 ImageEncoder ResNet 101](#221-imageencoder-resnet-101)\n",
    "    - [2.2.2 TextEncoder hidden size 12000](#222-textencoder-hidden-size-12000)\n",
    "    - [2.2.3 Fusion Model ResNet101 hidden size 12000](#223-fusion-model-resnet101-hidden-size-12000)\n",
    "- [3 Mints Image Only](#3-mints-image-only)\n",
    "  - [3.1 Resnet 18](#31-resnet-18)\n",
    "  - [3.2 ResNet50](#32-resnet50)\n",
    "  - [3.3 ResNet101](#33-resnet101)\n",
    "  - [3.4 ResNet 50 + Augments](#34-resnet-50-+-augments)\n",
    "  - [3.5 ResNet101 + Augemnts](#35-resnet101-+-augemnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a9531",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup\n",
    "\n",
    "---\n",
    "General Remarks:\n",
    "Some Modules have to be adjusted in order to work properly\n",
    "For all type models you have to change the defintion of TextEmbeddings in CoinDataset.\n",
    "For all Fusion models with another base than ResNet18 you have to change it in the constructur of \n",
    "the Fusion model class in the architectures modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d35f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ModelTrainer import ModelTrainer\n",
    "import torchvision\n",
    "from torchvision.models.resnet import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights, resnet101, ResNet101_Weights \n",
    "from torchvision.models import swin_v2_b ,Swin_V2_B_Weights\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Architectures import TextClassefier, Transformer_TextClassifier, Fusion, Fusion_From_Scratch\n",
    "# Check GPU support on your machine.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f657c2",
   "metadata": {},
   "source": [
    "# 1 Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98cbf51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = r\"F:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\\multimodal - Kopie\"\n",
    "model_save_path = dir +\"\\\\models\"\n",
    "\n",
    "# specifies image data \n",
    "train, val = dir+\"\\\\train.csv\", dir+\"\\\\val.csv\"\n",
    "\n",
    "# specifies language data\n",
    "train_emb, val_emb   = dir+'\\\\embeddings_train_alternative.npy', dir+'\\\\embeddings_val_alternative.npy'\n",
    "\n",
    "# specifies output feature size for imageencoder, textencoder and fusionmodel \n",
    "output_features = len(pd.read_csv(train, delimiter=',', skiprows=0, low_memory=False, encoding='iso-8859-1')[\"class\"].unique())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "set to true if you want to apply augments\n",
    "should increase performance slightly (roughly 2-3%)\n",
    "but also increases training time\n",
    "\"\"\"\n",
    "use_augments = False\n",
    "\n",
    "if use_augments:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])\n",
    "else:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c444e",
   "metadata": {},
   "source": [
    "## 1.1 ResNet18 Base\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f63c5b",
   "metadata": {},
   "source": [
    "## 1.1.1 ImageEncoder Resnet18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b64907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 95]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bac9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8103\n",
      "Validation Top-5 Accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8103, device='cuda:0', dtype=torch.float64), 0.9767741935483871)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelImageEncoder_ResNet18.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bcbc7",
   "metadata": {},
   "source": [
    "## 1.1.2 TextEncoder hidden size 10000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a0f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 95]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TextClassefier(input_size=1536, hidden_size=10000, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TextEncoder',\n",
    "            batch_size=4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811c3644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.8190\n",
      "Validation Top-5 Accuracy: 0.9947\n"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTextEncoder.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcac6b7",
   "metadata": {},
   "source": [
    "## 1.1.3 Fusion Model ResNet18 hidden size 10000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e6a2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 95]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "TextModel = TextClassefier(input_size=1536, hidden_size=10000, output_size=output_features)\n",
    "image_encoder = dir + \"\\\\models\\modelImageEncoder_ResNet18.tar\"\n",
    "text_encoder = dir + \"\\\\models\\modelTextEncoder.tar\"\n",
    "model = Fusion(image_encoder, text_encoder, TextModel, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TR0_Res18_types_with_des_78_multimodal',\n",
    "            batch_size=4,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d526834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.8441\n",
      "Validation Top-5 Accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR0_Res18_types_with_des_78_multimodal.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa63611",
   "metadata": {},
   "source": [
    "# 1.2 ResNet101 Base\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858c1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = r\"F:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\\top5_accs\\typesmodel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114c2c2",
   "metadata": {},
   "source": [
    "## 1.2.1 ImageEncoder ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5af31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 95]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952be4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8245\n",
      "Validation Top-5 Accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8245, device='cuda:0', dtype=torch.float64), 0.9819354838709677)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelImageEncoder_ResNet101.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a75d7",
   "metadata": {},
   "source": [
    "## 1.2.2 TextEncoder hidden size 12000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472495a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 95]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TextClassefier(input_size=1536, hidden_size=12000, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TextEncoder',\n",
    "            batch_size=4\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fc7ce",
   "metadata": {},
   "source": [
    "## 1.2.3 Fusion Model ResNet101 hidden size 12000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec96ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.8173\n",
      "Validation Top-5 Accuracy: 0.9952\n"
     ]
    }
   ],
   "source": [
    "# because of our dataset construction the behavior is non deterministic, so we average over ten evaluation runs\n",
    "Solver.load(model_save_path+\"\\\\modelTextEncoder.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab99336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2445\n",
      "torch.Size([1, 3, 299, 299]) torch.Size([1, 95]) torch.Size([1, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "TextModel = TextClassefier(input_size=1536, hidden_size=12000, output_size=output_features)\n",
    "image_encoder = model_save_path+\"\\\\modelImageEncoder_ResNet101.tar\"\n",
    "text_encoder = model_save_path+\"\\\\modelTextEncoder.tar\"\n",
    "model = Fusion(image_encoder, text_encoder, TextModel, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TR0_Res18_types_with_des_78_multimodal',\n",
    "            batch_size=4,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26e86ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 550.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 0 bytes free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Solver\u001b[39m.\u001b[39;49mload(model_save_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mmodelTR0_Res101_types_with_des_78_multimodal.tar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m Solver\u001b[39m.\u001b[39mevaluate(multimodal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39maccs = []\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mtop_5_accs = []\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mprint(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\\top5_accs\\ModelTrainer.py:223\u001b[0m, in \u001b[0;36mModelTrainer.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m# Load model and optimizer state.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(checkpoint\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mload_state_dict(checkpoint\u001b[39m.\u001b[39;49mpop(\u001b[39m'\u001b[39;49m\u001b[39moptimizer\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m    225\u001b[0m \u001b[39m# Load learning rate scheduler state if defined.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler:\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:379\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict\u001b[1;34m(self, state_dict)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Loads the optimizer state.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \n\u001b[0;32m    374\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[39m    state_dict (dict): optimizer state. Should be an object returned\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39m        from a call to :meth:`state_dict`.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[39m# deepcopy, to be consistent with module API\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m state_dict \u001b[39m=\u001b[39m deepcopy(state_dict)\n\u001b[0;32m    380\u001b[0m \u001b[39m# Validate the state_dict\u001b[39;00m\n\u001b[0;32m    381\u001b[0m groups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:118\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    110\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe default implementation of __deepcopy__() for wrapper subclasses \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39monly works for subclass types that implement clone() and for which \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdifferent type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m         )\n\u001b[0;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     new_storage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_typed_storage()\u001b[39m.\u001b[39;49m_deepcopy(memo)\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_quantized:\n\u001b[0;32m    120\u001b[0m         \u001b[39m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[0;32m    121\u001b[0m         quantizer_params: Union[\n\u001b[0;32m    122\u001b[0m             Tuple[torch\u001b[39m.\u001b[39mqscheme, \u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[0;32m    123\u001b[0m             Tuple[torch\u001b[39m.\u001b[39mqscheme, Tensor, Tensor, \u001b[39mint\u001b[39m],\n\u001b[0;32m    124\u001b[0m         ]\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\storage.py:661\u001b[0m, in \u001b[0;36mTypedStorage._deepcopy\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deepcopy\u001b[39m(\u001b[39mself\u001b[39m, memo):\n\u001b[1;32m--> 661\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_wrapped_storage(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_untyped_storage, memo))\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39m__deepcopy__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[39m=\u001b[39m copier(memo)\n\u001b[0;32m    154\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[39m=\u001b[39m dispatch_table\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\storage.py:98\u001b[0m, in \u001b[0;36m_StorageBase.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata \u001b[39min\u001b[39;00m memo:\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m memo[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata]\n\u001b[1;32m---> 98\u001b[0m new_storage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone()\n\u001b[0;32m     99\u001b[0m memo[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata] \u001b[39m=\u001b[39m new_storage\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m new_storage\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\storage.py:112\u001b[0m, in \u001b[0;36m_StorageBase.clone\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    111\u001b[0m     \u001b[39m\"\"\"Returns a copy of this storage\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnbytes(), device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 550.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 0 bytes free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR0_Res101_types_with_des_78_multimodal.tar\")\n",
    "Solver.evaluate(multimodal=True)\n",
    "\n",
    "\"\"\"\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ff46e",
   "metadata": {},
   "source": [
    "# 2 Mints Multimodal\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d04447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r\"F:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\\muldimodal- Mints\"\n",
    "model_save_path = dir +\"\\\\models\"\n",
    "\n",
    "# specifies image data \n",
    "train, val = dir+\"\\\\train.csv\", dir+\"\\\\val.csv\"\n",
    "\n",
    "# specifies language data\n",
    "train_emb, val_emb   = dir+'\\\\embeddings_train.npy', dir+'\\\\embeddings_val.npy'\n",
    "\n",
    "# specifies output feature size for imageencoder, textencoder and fusionmodel \n",
    "output_features = len(pd.read_csv(train, delimiter=',', skiprows=0, low_memory=False, encoding='iso-8859-1')[\"class\"].unique())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "set to true if you want to apply augments\n",
    "should increase performance slightly (roughly 2-3%)\n",
    "but also increases training time\n",
    "\"\"\"\n",
    "use_augments = False\n",
    "\n",
    "if use_augments:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])\n",
    "else:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb8719",
   "metadata": {},
   "source": [
    "## 2.1 ResNet18 Base\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202d8e8",
   "metadata": {},
   "source": [
    "## 2.1.1 ImageEncoder ResNet18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fc1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21008\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fac6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8126\n",
      "Validation Top-5 Accuracy: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8126, device='cuda:0', dtype=torch.float64), 0.9528383027522935)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelImageEncoder_ResNet18.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf213d2",
   "metadata": {},
   "source": [
    "## 2.1.2 TextEncoder hidden size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423ab04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21008\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 83]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = TextClassefier(input_size=1536, hidden_size=10000, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TextEncoder',\n",
    "            batch_size=4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eb7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.8817\n",
      "Validation Top-5 Accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTextEncoder.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185186d",
   "metadata": {},
   "source": [
    "## 2.1.3 Fusion Model Resnet18 10000 hidden size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089f9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21008\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 83]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "TextModel = TextClassefier(input_size=1536, hidden_size=10000, output_size=output_features)\n",
    "image_encoder = model_save_path+\"\\\\modelImageEncoder_ResNet18.tar\"\n",
    "text_encoder = model_save_path+\"\\\\modelTextEncoder.tar\"\n",
    "model = Fusion(image_encoder, text_encoder, TextModel, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TR0_Res18_types_with_des_78_multimodal',\n",
    "            batch_size=4,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcce5443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.9213\n",
      "Validation Top-5 Accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelFusion.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442eaf1d",
   "metadata": {},
   "source": [
    "# 2.2 ResNet101 Base\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fdc07",
   "metadata": {},
   "source": [
    "## 2.2.1 ImageEncoder ResNet 101\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b39291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = dir +\"\\\\models2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d5b5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21008\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfa08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8492\n",
      "Validation Top-5 Accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8492, device='cuda:0', dtype=torch.float64), 0.9683199541284404)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelImageEncoder_ResNet101.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b261ef4",
   "metadata": {},
   "source": [
    "## 2.2.2 TextEncoder hidden size 12000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093acec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21008\n",
      "torch.Size([4, 3, 299, 299]) torch.Size([4, 83]) torch.Size([4, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = TextClassefier(input_size=1536, hidden_size=12000, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TextEncoder',\n",
    "            batch_size=4\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73040b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This is the average:\n",
      "Validation Top-1 Accuracy: 0.8849\n",
      "Validation Top-5 Accuracy: 0.9713\n"
     ]
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTextEncoder.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cda69d",
   "metadata": {},
   "source": [
    "## 2.2.3 Fusion Model ResNet101 hidden size 12000\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9188e7e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 550.00 MiB (GPU 0; 4.00 GiB total capacity; 3.03 GiB already allocated; 0 bytes free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m image_encoder \u001b[39m=\u001b[39m model_save_path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodelImageEncoder_ResNet101.tar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m text_encoder \u001b[39m=\u001b[39m model_save_path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodelTextEncoder.tar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m Fusion(image_encoder, text_encoder, TextModel, output_size\u001b[39m=\u001b[39;49moutput_features)\n\u001b[0;32m      6\u001b[0m \u001b[39m# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \u001b[39;00m\n\u001b[0;32m      7\u001b[0m Solver \u001b[39m=\u001b[39m ModelTrainer(model\u001b[39m=\u001b[39mmodel,  \n\u001b[0;32m      8\u001b[0m             train_path\u001b[39m=\u001b[39mtrain, \n\u001b[0;32m      9\u001b[0m            train_embedding_path\u001b[39m=\u001b[39mtrain_emb, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m     16\u001b[0m             )\n",
      "File \u001b[1;32mf:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\\top5_accs\\Architectures.py:22\u001b[0m, in \u001b[0;36mFusion.__init__\u001b[1;34m(self, model_path, model_path2, TextClassefier, hidden_size, output_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_embedding_model \u001b[39m=\u001b[39m TextClassefier\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_embedding_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_embedding_model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 22\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(model_path2)\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_embedding_model\u001b[39m.\u001b[39mload_state_dict(checkpoint\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     24\u001b[0m \u001b[39m# we do not want to overwrite weights for the visual embedding model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[1;32mc:\\Users\\basti\\anaconda3\\lib\\site-packages\\torch\\_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\n\u001b[0;32m     82\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     84\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[0;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 550.00 MiB (GPU 0; 4.00 GiB total capacity; 3.03 GiB already allocated; 0 bytes free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "TextModel = TextClassefier(input_size=1536, hidden_size=12000, output_size=output_features)\n",
    "image_encoder = model_save_path+\"\\\\modelImageEncoder_ResNet101.tar\"\n",
    "text_encoder = model_save_path+\"\\\\modelTextEncoder.tar\"\n",
    "model = Fusion(image_encoder, text_encoder, TextModel, output_size=output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train, \n",
    "           train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='TR0_Res18_types_with_des_78_multimodal',\n",
    "            batch_size=4,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0eca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelFusion.tar\")\n",
    "accs = []\n",
    "top_5_accs = []\n",
    "\n",
    "for i in range(10):\n",
    "    acc, top_5_acc = Solver.evaluate(multimodal=True, show=False)\n",
    "    accs.append(acc.cpu().numpy())\n",
    "    top_5_accs.append(top_5_acc)\n",
    "print('\\n This is the average:')    \n",
    "print(f'Validation Top-1 Accuracy: {np.array(accs).sum()/10:.4f}')\n",
    "print(f'Validation Top-5 Accuracy: {np.array(top_5_accs).sum()/10:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb3629",
   "metadata": {},
   "source": [
    "# 3 Mints Image Only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9a069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r\"F:\\Users\\basti\\Documents\\Goethe Uni\\Data Challange\"\n",
    "model_save_path = dir +\"\\\\models\"\n",
    "\n",
    "# specifies image data \n",
    "train, val = dir+\"\\\\train.csv\", dir+\"\\\\val.csv\"\n",
    "\n",
    "# specifies language data\n",
    "train_emb, val_emb   = dir+'\\\\embeddings_train.npy', dir+'\\\\embeddings_val.npy'\n",
    "\n",
    "# specifies output feature size for imageencoder, textencoder and fusionmodel \n",
    "output_features = len(pd.read_csv(train, delimiter=',', skiprows=0, low_memory=False, encoding='iso-8859-1')[\"class\"].unique())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "set to true if you want to apply augments\n",
    "should increase performance slightly (roughly 2-3%)\n",
    "but also increases training time\n",
    "\"\"\"\n",
    "use_augments = False\n",
    "\n",
    "if use_augments:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])\n",
    "else:\n",
    "    train_augmentations = transforms.Compose([\n",
    "    transforms.ToTensor(),  # convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # normalize images\n",
    "    transforms.Resize((299, 299))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ebef2",
   "metadata": {},
   "source": [
    "## 3.1 Resnet 18\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20b0fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21593\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ee9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8058\n",
      "Validation Top-5 Accuracy: 0.9482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8058, device='cuda:0', dtype=torch.float64), 0.9481553940749021)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR0_Res18.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38121a5c",
   "metadata": {},
   "source": [
    "## 3.2 ResNet50\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf190f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR0_Res50.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110aeb3",
   "metadata": {},
   "source": [
    "## 3.3 ResNet101\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77128c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21593\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981dfbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8291\n",
      "Validation Top-5 Accuracy: 0.9584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8291, device='cuda:0', dtype=torch.float64), 0.9583566238121856)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR0_Res101.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6888a",
   "metadata": {},
   "source": [
    "## 3.4 ResNet 50 + Augments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbc7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21593\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf7009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8432\n",
      "Validation Top-5 Accuracy: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8432, device='cuda:0', dtype=torch.float64), 0.9652040245947456)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR5_Res50.tar\")\n",
    "Solver.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a9898",
   "metadata": {},
   "source": [
    "## 3.5 ResNet101 + Augemnts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d449bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "21593\n",
      "torch.Size([16, 3, 299, 299]) torch.Size([16, 83]) torch.Size([16, 1536])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, output_features)\n",
    "\n",
    "# hyperparameters can be set here. Check ModelTrainer.py for details. If not specified differently default values of the Modeltrainer have been used \n",
    "Solver = ModelTrainer(model=model,  \n",
    "            train_path=train,\n",
    "            train_embedding_path=train_emb, \n",
    "            val_path=val,\n",
    "            val_embedding_path=val_emb, \n",
    "            train_augmentations=train_augmentations,\n",
    "            save_path=model_save_path,\n",
    "            postfix='ImageEncoder_ResNet18',\n",
    "            batch_size=16\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0867b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Top-1 Accuracy: 0.8569\n",
      "Validation Top-5 Accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8569, device='cuda:0', dtype=torch.float64), 0.9700950251537171)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solver.load(model_save_path+\"\\\\modelTR5_Res101.tar\")\n",
    "Solver.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
