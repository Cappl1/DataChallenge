{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbae843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9080cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File generated with image_to_csv.py\n",
    "df_img=pd.read_csv(r\"C:/Users/gar43/OneDrive/Documents/DataChallenge/dataset_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b988dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs are the last numbers in image_name\n",
    "def find_id(x):\n",
    "    pattern = r\"_([\\d]+)_[a-zA-Z]\\.jpg\"\n",
    "    return re.search(pattern, x).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f06e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img['ID'] = df_img['image_name'].apply(lambda x: find_id(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3564c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately IDs are not unique, \n",
    "# but we need them to match with coin discriptions.\n",
    "df_img = df_img.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9087f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin descriptions.\n",
    "df_descrip=pd.read_csv(r\"C:/Users/gar43/OneDrive/Documents/DataChallenge/CN_coin_descriptions.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd0e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split thw whole string in dataframe and find the length of the resulted list\n",
    "a =df_descrip[0].apply(lambda x:x.split(',\"'))\n",
    "m=a.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01b68e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 3850,  7495,  8396,  8398,  8400, 12908, 17846, 17847, 19138,\n",
       "            23813, 23814, 23988, 31815, 31816, 33763, 33764, 33767, 33768,\n",
       "            33769, 33770, 33774, 33775, 33777, 33778, 35085, 35087, 35089,\n",
       "            35091, 35093, 35095, 40142],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find incorrect and incomplete data records\n",
    "m[m<3].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ec38a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u2002\"']\n",
      "['\\u2002\"', 'Forepart of winged horse, right.\"']\n"
     ]
    }
   ],
   "source": [
    "# Examples of incorrect strings\n",
    "print(a[35095])\n",
    "print(a[33775])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b568547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop incorrect data recrords \n",
    "a = a.drop(m[m<3].index, axis=0)\n",
    "df_descrip = df_descrip.drop(m[m<3].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a62dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descrip['ID'] = [x[0] for x in a]\n",
    "df_descrip['obverse '] = [x[1] for x in a]\n",
    "df_descrip['reverse '] = [x[2] for x in a]\n",
    "df_descrip=df_descrip.drop(columns=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a8b029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_descrip[df_descrip['ID'].duplicated()]['ID'].tolist()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477abbe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged = df_descrip.merge(df_img, on='ID', how='left')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedc2a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Not all classes/types have descriprion\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged['class'] = df_merged['class'].astype('int')\n",
    "# Manually correct datapoint\n",
    "df_merged['class'][0] = 3987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeac524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_csv = df_merged[['filename', 'image_name', 'class']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cda76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exract unique descriptions for promts\n",
    "df_for_promts = df_merged.drop_duplicates(subset=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8bc90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "begin = 'Coin Obverse shows '\n",
    "end = 'Reverse shows '\n",
    "df_for_promts['promt'] = begin \\\n",
    "                        + df_for_promts['obverse '] \\\n",
    "                        + end \\\n",
    "                        + df_for_promts['reverse ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd87234",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop all descriptions that have more/equal than 77 tokens, because 77 tokens are max for clip\n",
    "import clip_clip\n",
    "df_for_promts['len'] = df_for_promts['promt'].apply(lambda x: clip_clip.tokenize_len(x))\n",
    "df_for_promts_77 = df_for_promts.drop(df_for_promts[df_for_promts['len']>76].index)\n",
    "\n",
    "# Drop also from img_csv\n",
    "more_than_76_promts = df_for_promts[df_for_promts['len']>76]\n",
    "img_csv = img_csv[~img_csv['class'].isin(more_than_76_promts['class'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4cad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IDs to 0 to N for one hot encoding\n",
    "df_for_promts_77['class_id'] = df_for_promts_77['class'].rank(method='dense').astype(int) - 1\n",
    "img_csv =img_csv.merge(df_for_promts_77[['class', 'class_id']], on='class', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4d233f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write an input file\n",
    "img_csv[['filename', 'image_name', 'class_id']].to_csv('clip_img_77_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0002ad00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# The CPU should be sufficient for this task\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "767f2e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9178"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_csv['class_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e21b5bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=336, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(336, 336))\n",
       "    <function _convert_image_to_rgb at 0x000002178AB4B250>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, download_root=None)\n",
    "model.eval()\n",
    "\n",
    "# This also provides a useful preprocessing pipeline for the images\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ddfb670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 336, 336]) torch.Size([8, 9178])\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from CoinDataset_clip import CoinDataset\n",
    "images = CoinDataset('clip_img_77_id.csv', preprocess, 9178 )\n",
    "img_dataloader = DataLoader(images, batch_size=8, shuffle=False, num_workers=6)\n",
    "for i, batch in enumerate(img_dataloader):\n",
    "    x, y = batch[\"image\"], batch[\"label\"]\n",
    "    print(x.shape, y.shape)\n",
    "    break\n",
    "print(\"data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e0547e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with promts\n",
    "promts = df_for_promts_77['promt'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ec8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompts with `clip.tokenize`\n",
    "tokenized = torch.cat([clip.tokenize(c) for c in promts]).to(device)\n",
    "\n",
    "#encoder is on gpu so we have to put inputs to gpu\n",
    "tokenized = tokenized.to(device)\n",
    "# The result is a tensor of shape (1024, 10),\n",
    "# since we have 10 classes and the feature dimension of the text encoder is 1024\n",
    "# we don't want to calculate gradient during evaluation\n",
    "\n",
    "text_embedding = torch.zeros((768,9178)).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(90):\n",
    "        text_embedding[:,i*100:i*100+100] = model.encode_text(tokenized[i*100:i*100+100,:]).permute(1,0)\n",
    "with torch.no_grad():\n",
    "    text_embedding[:,9000:9179] = model.encode_text(tokenized[9000:9179,:]).permute(1,0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0950224b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 9178])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd4d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "Accuracy:  0.000\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "##                   START OF YOUR CODE                   ##\n",
    "############################################################\n",
    "correct = 0\n",
    "total = 25613\n",
    "\n",
    "with torch.no_grad():\n",
    "    #1. Loop over the dataset and put stuff to gpu cause our model is on gpu\n",
    "    for i, batch in enumerate(img_dataloader):\n",
    "        \n",
    "        if i%500 ==0:\n",
    "            print(i)\n",
    "        inputs = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        \n",
    "        # 2. Create visual embeddings with the image encoder\n",
    "        visual_embedding = model.encode_image(inputs)\n",
    "        \n",
    "        # 3. Calculate the cosine similarity between the image and text embeddings (Note Images are batched from Dl)\n",
    "        cosine_similarity = visual_embedding @ text_embedding.half()\n",
    "        \n",
    "        # image norms along dim 1 because visual embedding has shape [batch, embbeding_size]\n",
    "        image_norms = torch.norm(visual_embedding, p=2, dim=1).unsqueeze(1) \n",
    "        # text norms along dim 0 because it has shape [embedding size, num_classes]\n",
    "        text_norms = torch.norm(text_embedding, p=2, dim=0).unsqueeze(0)\n",
    "        \n",
    "        #because of unsquezzing before we have: image_norms[64,1] and text_norms[1,10] and with the help of broadcasting we get \n",
    "        #all relevant norm products\n",
    "        cosine_similarity = cosine_similarity / (image_norms * text_norms)\n",
    "\n",
    "        #prediction is equal to the position of highest cosine similarity\n",
    "        preds = torch.argmax(cosine_similarity, dim=1)\n",
    "        \n",
    "        #_, top5_indices = torch.topk(cosine_similarity, 5, dim=1)\n",
    "        labels = torch.argmax(labels, dim = 1) \n",
    "\n",
    "        # Increment the number of correct predictions based on the comparison between the predicted and actual labels\n",
    "        correct += sum(preds == labels)\n",
    "        #correct += sum(torch.sum(top5_indices == labels.unsqueeze(1), dim=1))\n",
    "\n",
    "        \n",
    "        \n",
    "print(f\"Accuracy: {correct / total: .3f}\")\n",
    "############################################################\n",
    "##                    END OF YOUR CODE                    ##\n",
    "############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
